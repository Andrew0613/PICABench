#!/usr/bin/env python3
# physedit_eval_vllm.py
# PhysEditè¯„ä¼°è„šæœ¬ï¼ŒåŸºäºvLLMè¿›è¡Œæ‰¹é‡æ¨ç†
# æ”¯æŒqa_pairså’Œannotated_qa_pairsä¸¤ç§æ¨¡å¼
# æ”¯æŒå¯è§†åŒ–åŠŸèƒ½ï¼ˆdraw_boxå’Œcrop_boxæ¨¡å¼ï¼‰

import multiprocessing as mp
if mp.get_start_method(allow_none=True) != "spawn":
    mp.set_start_method("spawn", force=True)

import os
import sys
import json
import argparse
import random
import re
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple, Optional, Iterable
from PIL import Image, ImageDraw
import PIL
from tqdm import tqdm

from vllm import LLM, SamplingParams
from transformers import AutoProcessor

IMAGE_PATH_KEYS: Tuple[str, ...] = ("output_path", "output_image_path", "output_img_path")
JSON_PATTERN = re.compile(r'\{[^{}]*"answer"[^{}]*"explanation"[^{}]*\}', re.IGNORECASE | re.DOTALL)


@dataclass
class QATask:
    item_index: int
    qa_field: str
    qa_type: str
    qa_index: int
    image_path: str
    question: str
    answer: str
    source: str

def resolve_image_path(item: Dict[str, Any], base_dir: str) -> Optional[str]:
    """æ ¹æ®å¸¸è§å­—æ®µæ¨å¯¼å›¾ç‰‡å®Œæ•´è·¯å¾„"""
    for key in IMAGE_PATH_KEYS:
        rel_path = item.get(key)
        if rel_path:
            return os.path.join(base_dir, rel_path)
    return None

def iter_qa_entries(container: Any, default_type: str) -> Iterable[Tuple[str, int, Dict[str, Any]]]:
    """ç»Ÿä¸€éå†é—®ç­”ç»“æ„ï¼Œå…¼å®¹dict/listä¸¤ç§æ ¼å¼"""
    if isinstance(container, dict):
        for qa_type, qa_list in container.items():
            for idx, qa in enumerate(qa_list):
                yield qa_type, idx, qa
    elif isinstance(container, list):
        for idx, qa in enumerate(container):
            yield default_type, idx, qa

def draw_box_on_image(image: Image.Image, box_info: Dict, box_color="red") -> Image.Image:
    """åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶bounding box"""
    img_copy = image.copy()
    draw = ImageDraw.Draw(img_copy)
    
    # è·å–boxåæ ‡
    x = box_info.get("x", 0)
    y = box_info.get("y", 0)
    width = box_info.get("width", 0)
    height = box_info.get("height", 0)
    
    # ç»˜åˆ¶çŸ©å½¢æ¡†
    bbox = [x, y, x + width, y + height]
    draw.rectangle(bbox, outline=box_color, width=3)
    
    return img_copy

def resize_image(image: Image.Image) -> Image.Image:
    """å°†å›¾ç‰‡ç­‰æ¯”ä¾‹resizeåˆ°é•¿è¾¹ä¸º1024"""
    width, height = image.size
    long_edge = max(width, height)
    
    if long_edge == 1024:
        return image
    
    scale = 1024.0 / long_edge
    new_width = int(width * scale)
    new_height = int(height * scale)
    
    return image.resize((new_width, new_height), Image.LANCZOS)

def crop_image_with_box(image: Image.Image, box_info: Dict, padding=20) -> Image.Image:
    """æ ¹æ®boxåæ ‡è£å‰ªå›¾ç‰‡"""
    x = box_info.get("x", 0)
    y = box_info.get("y", 0)
    width = box_info.get("width", 0)
    height = box_info.get("height", 0)
    
    # æ·»åŠ padding
    x1 = max(0, x - padding)
    y1 = max(0, y - padding)
    x2 = min(image.size[0], x + width + padding)
    y2 = min(image.size[1], y + height + padding)
    cropped_image = image.crop((x1, y1, x2, y2))
    return cropped_image

def generate_viz_filename(item_index: int, qa_type: str, question_index: int, viz_mode: str) -> str:
    """ç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶å"""
    qa_type_short = qa_type.replace(" ", "_").replace("QA", "qa")
    return f"{item_index:04d}_{qa_type_short}_{question_index:03d}_{viz_mode}.jpg"

def create_visualization_and_question(output_img_path: str, qa_info: Dict, item_index: int, 
                                     qa_type: str, qa_index: int, viz_mode: str, 
                                     viz_dir: str, args) -> Tuple[str, str]:
    """åˆ›å»ºå¯è§†åŒ–å›¾ç‰‡å¹¶è¿”å›å¤„ç†åçš„é—®é¢˜æ–‡æœ¬"""
    question = qa_info.get("question", "")
    box_info = qa_info.get("box", {})
    
    # ç”Ÿæˆæ–‡ä»¶åå’Œè·¯å¾„
    viz_filename = generate_viz_filename(item_index, qa_type, qa_index, viz_mode)
    viz_path = os.path.join(viz_dir, viz_filename)
    # ç›¸å¯¹è·¯å¾„éœ€è¦ä¸ä¸Šé¢çš„viz_dirä¸€è‡´ï¼Œå¦åˆ™åç»­åŠ è½½ä¼šæ‰¾ä¸åˆ°æ–‡ä»¶è€Œå›é€€ä¸ºç™½å›¾
    viz_rel_path = os.path.join(f"visualization_annotated_qa_{viz_mode}", viz_filename)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(viz_dir, exist_ok=True)
    
    try:
        # åŠ è½½å›¾ç‰‡å¹¶resizeåˆ°é•¿è¾¹1024ï¼ˆå› ä¸ºbboxåæ ‡æ˜¯åœ¨é•¿è¾¹1024ä¸‹æ ‡æ³¨çš„ï¼‰
        image = Image.open(output_img_path).convert("RGB")
        image_resized = resize_image(image)
        
        if viz_mode == "draw_box":
            # ç”»æ¡†æ¨¡å¼ï¼šä¿ç•™åŸå§‹æé—®ï¼Œä»…åœ¨å›¾ä¸Šç”»æ¡†
            vllm_question = question
            viz_image = draw_box_on_image(image_resized, box_info, args.box_color)
        elif viz_mode == "crop_box":
            # è£å‰ªæ¨¡å¼ï¼šä¿ç•™åŸå§‹æé—®ï¼Œè£å‰ªå›¾ç‰‡
            vllm_question = question
            viz_image = crop_image_with_box(image_resized, box_info, args.viz_padding)
        elif viz_mode == "crop_box_and_resize":
            # è£å‰ªæ¨¡å¼ï¼šä¿ç•™åŸå§‹æé—®ï¼Œè£å‰ªå›¾ç‰‡å†æ‹‰ä¼¸
            vllm_question = question
            viz_image = crop_image_with_box(image_resized, box_info, args.viz_padding)
            viz_image = resize_image(viz_image)
        else:
            raise ValueError(f"Unknown viz_mode: {viz_mode}")
        
        # ä¿å­˜å¯è§†åŒ–å›¾ç‰‡
        viz_image.save(viz_path, quality=90)
        
        # è®°å½•é—®é¢˜ä¿®æ”¹æƒ…å†µ
        if args.log_question_changes and question != vllm_question:
            print(f"Question modified ({viz_mode}): {question} -> {vllm_question}")
        
        return vllm_question, viz_rel_path
        
    except Exception as e:
        print(f"Error creating visualization for {viz_path}: {e}")
        return question, ""

def parse_structured_response(response: str) -> Tuple[str, str]:
    """è§£æç»“æ„åŒ–çš„JSONå›ç­”"""
    return _parse_json_response(response) or _fallback_parse_response(response)

def _parse_json_response(response: str) -> Tuple[str, str] | None:
    """å°è¯•è§£æJSONæ ¼å¼çš„å›ç­”"""
    try:
        response_clean = response.strip()
        
        # å¤šç§æ–¹å¼æŸ¥æ‰¾JSON
        json_candidates = []
        
        # æ–¹æ³•1: æŸ¥æ‰¾å®Œæ•´çš„{}å—
        json_start = response_clean.find('{')
        json_end = response_clean.rfind('}') + 1
        if json_start != -1 and json_end > json_start:
            json_candidates.append(response_clean[json_start:json_end])
        
        # æ–¹æ³•2: ä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…JSON
        json_candidates.extend(JSON_PATTERN.findall(response_clean))
        
        # å°è¯•è§£ææ¯ä¸ªå€™é€‰JSON
        for json_str in json_candidates:
            try:
                data = json.loads(json_str)
                answer = data.get("answer", "").strip()
                explanation = data.get("explanation", "").strip()
                
                if answer:  # åªè¦æœ‰answerå°±è®¤ä¸ºè§£ææˆåŠŸ
                    return _normalize_answer(answer), explanation
            except json.JSONDecodeError:
                continue
                
    except Exception:
        pass
    
    return None

def _normalize_answer(answer: str) -> str:
    """æ ‡å‡†åŒ–ç­”æ¡ˆæ ¼å¼"""
    answer_lower = answer.lower().strip().rstrip('.')
    if answer_lower in ["yes", "y", "true"]:
        return "Yes"
    elif answer_lower in ["no", "n", "false"]:
        return "No"
    return answer.strip()

def _fallback_parse_response(response: str) -> Tuple[str, str]:
    """å¤‡ç”¨è§£ææ–¹æ³•"""
    return extract_yes_no_answer(response), ""

def extract_yes_no_answer(response: str) -> str:
    """ä»æ¨¡å‹å›å¤ä¸­æå–Yes/Noç­”æ¡ˆï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
    response_lower = response.lower().strip()
    
    # ä¼˜å…ˆåŒ¹é…å¼€å¤´çš„Yes/No
    if response_lower.startswith("yes"):
        return "Yes"
    elif response_lower.startswith("no"):
        return "No"
    
    # æŸ¥æ‰¾å¥å­ä¸­çš„Yes/No
    if re.search(r'\byes\b', response_lower):
        return "Yes"
    elif re.search(r'\bno\b', response_lower):
        return "No"
    
    # é»˜è®¤è¿”å›åŸå§‹å›å¤çš„å‰10ä¸ªå­—ç¬¦
    return response[:10] if response else "Unknown"

def init_llm(model_path: str, tp: int, dtype: str, gpu_util: float, max_len: int):
    """åˆå§‹åŒ–vLLMå¼•æ“"""
    llm = LLM(
        model=model_path,
        tensor_parallel_size=tp,
        dtype=dtype,
        gpu_memory_utilization=gpu_util,
        trust_remote_code=True,
        max_model_len=max_len,
    )
    return llm

def create_structured_message(msgs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ä¸ºæ¶ˆæ¯æ·»åŠ ç»“æ„åŒ–å›ç­”æŒ‡ä»¤"""
    structured_msg = msgs.copy()
    if not structured_msg or structured_msg[-1]["role"] != "user":
        return structured_msg
    
    # ç»“æ„åŒ–å›ç­”æŒ‡ä»¤
    json_instruction = "\n\nPlease provide a structured answer in the following JSON format:\n{\"answer\": \"Yes\" or \"No\", \"explanation\": \"Brief explanation of your reasoning\"}\n\nOutput ONLY valid JSON. No extra text."
    
    original_content = structured_msg[-1]["content"]
    if isinstance(original_content, list):
        # å¤šæ¨¡æ€å†…å®¹
        structured_content = original_content + [{"type": "text", "text": json_instruction}]
    else:
        # çº¯æ–‡æœ¬å†…å®¹
        structured_content = original_content + json_instruction
    
    structured_msg[-1]["content"] = structured_content
    return structured_msg

def prepare_vllm_batch(qa_tasks: List[QATask]) -> Tuple[List[List[Dict[str, Any]]], List[List[Image.Image]]]:
    """å‡†å¤‡vLLMæ‰¹å¤„ç†çš„æ¶ˆæ¯å’Œå›¾ç‰‡"""
    msgs_batch: List[List[Dict[str, Any]]] = []
    imgs_batch: List[List[Image.Image]] = []
    pil_cache: Dict[str, Image.Image] = {}
    
    for task in tqdm(qa_tasks, desc="å‡†å¤‡å›¾ç‰‡"):
        if task.image_path not in pil_cache:
            try:
                img = Image.open(task.image_path).convert("RGB")
                img = resize_image(img)
                pil_cache[task.image_path] = img
            except Exception as e:
                print(f"Error loading image {task.image_path}: {e}")
                pil_cache[task.image_path] = Image.new("RGB", (512, 512), "white")
        img = pil_cache[task.image_path]
        msgs_batch.append([{"role": "user", "content": [{"type": "image", "image": img}, {"type": "text", "text": task.question}]}])
        imgs_batch.append([img])
    
    return msgs_batch, imgs_batch

def build_vllm_inputs(processor: AutoProcessor, 
                      batch_msgs: List[List[Dict[str, Any]]], 
                      batch_images: List[List[Image.Image]]) -> List[Dict[str, Any]]:
    """æ„å»ºvLLMè¾“å…¥"""
    # æ·»åŠ ç»“æ„åŒ–å›ç­”æŒ‡ä»¤
    structured_msgs = [create_structured_message(msgs) for msgs in batch_msgs]
    
    # ç”Ÿæˆprompts
    prompts = [
        processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        for msgs in structured_msgs
    ]
    
    # æ„é€ è¾“å…¥
    return [
        {"prompt": text, "multi_modal_data": {"image": imgs}}
        for text, imgs in zip(prompts, batch_images)
    ]

def vllm_generate_batch(llm: LLM, processor: AutoProcessor, 
                        batch_msgs: List[List[Dict[str, Any]]], 
                        batch_images: List[List[Image.Image]], 
                        max_new_tokens: int) -> List[str]:
    """æ‰¹é‡æ¨ç†"""
    # æ„å»ºè¾“å…¥
    inputs = build_vllm_inputs(processor, batch_msgs, batch_images)
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sp = SamplingParams(
        temperature=0.0,
        top_p=1.0,
        max_tokens=max_new_tokens,
        stop=["ï¿½ï¿½", "\nï¿½ï¿½", "ğŸ“\n"]
    )
    
    # æ‰¹é‡æ¨ç†
    outputs = llm.generate(inputs, sp)
    return [o.outputs[0].text if o.outputs else "" for o in outputs]

def get_qa_entry(item: Dict[str, Any], qa_field: str, qa_type: str, qa_index: int) -> Dict[str, Any]:
    """å®šä½éœ€è¦æ›´æ–°çš„QAæ¡ç›®"""
    container = item.get(qa_field)
    if container is None and qa_field == "qa_pairs":
        container = item.get("annotated_qa_pairs")
    if isinstance(container, dict):
        return container[qa_type][qa_index]
    if isinstance(container, list):
        return container[qa_index]
    raise KeyError(f"QA entry not found for field={qa_field}, type={qa_type}, index={qa_index}")


def extract_qa_tasks_standard(items: List[Dict[str, Any]], image_base_dir: str) -> List[QATask]:
    """æå–qa_pairsæ¨¡å¼çš„é—®ç­”ä»»åŠ¡"""
    qa_tasks: List[QATask] = []
    for item_idx, item in enumerate(items):
        image_path = resolve_image_path(item, image_base_dir)
        if not image_path:
            continue
        qa_container = item.get("qa_pairs")
        if qa_container is None:
            qa_container = item.get("annotated_qa_pairs", {})
        for qa_type, qa_idx, qa in iter_qa_entries(qa_container, "qa"):
            question = qa.get("question")
            answer = qa.get("answer")
            if question and answer:
                qa_tasks.append(QATask(
                    item_index=item_idx,
                    qa_field="qa_pairs",
                    qa_type=qa_type,
                    qa_index=qa_idx,
                    image_path=image_path,
                    question=question,
                    answer=answer,
                    source="original"
                ))
    return qa_tasks

def extract_qa_tasks_annotated(items: List[Dict[str, Any]], image_base_dir: str, viz_mode: str, args) -> List[QATask]:
    """æå–annotated_qa_pairsæ¨¡å¼çš„é—®ç­”ä»»åŠ¡å¹¶ç”Ÿæˆå¯è§†åŒ–"""
    qa_tasks: List[QATask] = []
    viz_dir = os.path.join(image_base_dir, f"visualization_annotated_qa_{viz_mode}")
    for item_idx, item in enumerate(items):
        image_path = resolve_image_path(item, image_base_dir)
        if not image_path:
            continue
        qa_container = item.get("annotated_qa_pairs", {})
        for qa_type, qa_idx, qa in iter_qa_entries(qa_container, "annotated_qa"):
            question = qa.get("question")
            answer = qa.get("answer")
            if not (question and answer):
                continue
            vllm_question, viz_rel_path = create_visualization_and_question(
                image_path, qa, item_idx, qa_type, qa_idx, viz_mode, viz_dir, args
            )
            viz_path = os.path.join(image_base_dir, viz_rel_path) if viz_rel_path else image_path
            source = "visualization" if viz_rel_path else "original"
            qa_tasks.append(QATask(
                item_index=item_idx,
                qa_field="annotated_qa_pairs",
                qa_type=qa_type,
                qa_index=qa_idx,
                image_path=viz_path,
                question=vllm_question,
                answer=answer,
                source=source
            ))
    return qa_tasks

def evaluate_physedit_with_vllm(items: List[Dict[str, Any]], image_base_dir: str, 
                                processor: AutoProcessor, llm: LLM, args) -> List[Dict[str, Any]]:
    """ä¸»è¯„ä¼°å‡½æ•°"""
    if args.max_num is not None:
        items = items[:args.max_num]
    
    if args.qa_field == "qa_pairs":
        qa_tasks = extract_qa_tasks_standard(items, image_base_dir)
    elif args.qa_field == "annotated_qa_pairs":
        qa_tasks = extract_qa_tasks_annotated(items, image_base_dir, args.viz_mode, args)
    else:
        raise ValueError(f"Unsupported qa_field: {args.qa_field}")
    
    if not qa_tasks:
        print("No QA tasks found!")
        return items
    
    print(f"Found {len(qa_tasks)} QA tasks")
    msgs_batch, imgs_batch = prepare_vllm_batch(qa_tasks)
    print("å¼€å§‹VLMæ¨ç†...")
    answers = vllm_generate_batch(llm, processor, msgs_batch, imgs_batch, args.max_new_tokens)
    
    for task, model_response in zip(qa_tasks, answers):
        model_answer, model_explanation = parse_structured_response(model_response)
        gt_clean = task.answer.lower().strip().rstrip('.')
        model_clean = model_answer.lower().strip().rstrip('.')
        is_correct = gt_clean == model_clean
        qa_entry = get_qa_entry(items[task.item_index], task.qa_field, task.qa_type, task.qa_index)
        qa_entry["model_answer"] = model_answer
        qa_entry["model_response"] = model_response
        qa_entry["model_explanation"] = model_explanation
        qa_entry["is_correct"] = is_correct
        if task.qa_field == "annotated_qa_pairs" and task.source == "visualization":
            viz_rel_path = os.path.relpath(task.image_path, args.image_base_dir)
            qa_entry["visualization_path"] = viz_rel_path
            qa_entry["viz_mode"] = args.viz_mode
    
    return items

def calculate_accuracy_by_dimension(items: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è®¡ç®—å„ç»´åº¦å‡†ç¡®ç‡ï¼ˆæŒ‰æ ·æœ¬å¹³å‡ï¼‰"""
    total_questions = 0
    sample_acc_sum = 0.0
    sample_count = 0
    category_stats = {}
    law_stats = {}
    operation_stats = {}
    
    def collect_qas(item: Dict[str, Any]) -> List[Dict[str, Any]]:
        qa_sources: List[Dict[str, Any]] = []
        for field, default_type in (("qa_pairs", "qa"), ("annotated_qa_pairs", "annotated_qa")):
            container = item.get(field)
            if not container:
                continue
            for _, _, qa in iter_qa_entries(container, default_type):
                qa_sources.append(qa)
        return qa_sources
    
    def update_stat(stats: Dict, key: str, sample_acc: float, qa_total: int):
        if key not in stats:
            stats[key] = {"sum_acc": 0.0, "sample_count": 0, "qa_total": 0}
        stats[key]["sum_acc"] += sample_acc
        stats[key]["sample_count"] += 1
        stats[key]["qa_total"] += qa_total
    
    for item in items:
        category = item.get("physics_category", "unknown")
        law = item.get("physics_law", "unknown")
        operation = item.get("edit_operation", "unknown")
        
        qa_sources = collect_qas(item)
        sample_total = 0
        sample_correct = 0
        for qa in qa_sources:
            if "is_correct" in qa:
                sample_total += 1
                if qa["is_correct"]:
                    sample_correct += 1
        if sample_total == 0:
            continue
        
        sample_acc = sample_correct / sample_total
        sample_count += 1
        sample_acc_sum += sample_acc
        total_questions += sample_total
        
        update_stat(category_stats, category, sample_acc, sample_total)
        update_stat(law_stats, law, sample_acc, sample_total)
        update_stat(operation_stats, operation, sample_acc, sample_total)
    
    def calc_accuracy(stats: Dict) -> Dict:
        result = {}
        for key, value in stats.items():
            if value["sample_count"] > 0:
                acc = 100.0 * value["sum_acc"] / value["sample_count"]
            else:
                acc = 0.0
            result[key] = {
                "accuracy": acc,
                "sample_count": value["sample_count"],
                "qa_total": value["qa_total"]
            }
        return result
    
    overall_accuracy = (100.0 * sample_acc_sum / sample_count) if sample_count > 0 else 0.0
    return {
        "overall_accuracy": overall_accuracy,
        "sample_count": sample_count,
        "qa_total": total_questions,
        "by_category": calc_accuracy(category_stats),
        "by_law": calc_accuracy(law_stats),
        "by_operation": calc_accuracy(operation_stats)
    }

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_json_path", type=str, required=True,
                       help="è¾“å…¥çš„meta_info.jsonæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--image_base_dir", type=str, default=None,
                       help="å›¾ç‰‡åŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä¸ºè¾“å…¥JSONçš„ç›®å½•")
    parser.add_argument("--model_path", type=str, default="pretrained/Qwen/Qwen2.5-VL-72B-Instruct",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--qa_field", type=str, default="annotated_qa_pairs", 
                       choices=["qa_pairs", "annotated_qa_pairs"],
                       help="é€‰æ‹©ä½¿ç”¨qa_pairsè¿˜æ˜¯annotated_qa_pairså­—æ®µ")
    parser.add_argument("--viz_mode", type=str, default="crop_box_and_resize", 
                       choices=["draw_box", "crop_box", "crop_box_and_resize"],
                       help="å¯è§†åŒ–æ¨¡å¼ï¼ˆä»…annotated_qa_pairsæ¨¡å¼æœ‰æ•ˆï¼‰")
    parser.add_argument("--tensor_parallel_size", type=int, default=4,
                       help="å¼ é‡å¹¶è¡Œå¤§å°")
    parser.add_argument("--dtype", type=str, default="bfloat16", choices=["bfloat16","float16"])
    parser.add_argument("--gpu_mem_util", type=float, default=0.9)
    parser.add_argument("--max_model_len", type=int, default=5120)
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--img_size", type=int, default=1024, choices=[512, 1024])
    parser.add_argument("--max_num", type=int, default=None, help="æœ€å¤§å¤„ç†æ¡æ•°")
    parser.add_argument("--viz_padding", type=int, default=20, help="è£å‰ªæ¨¡å¼çš„paddingåƒç´ ")
    parser.add_argument("--box_color", default="red", help="ç»˜åˆ¶æ¡†çš„é¢œè‰²")
    parser.add_argument("--log_question_changes", action="store_true", 
                       help="è®°å½•é—®é¢˜æ–‡æœ¬çš„ä¿®æ”¹æƒ…å†µ")
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤image_base_dir
    if args.image_base_dir is None:
        args.image_base_dir = os.path.dirname(args.input_json_path)
    
    # è¯»å–æ•°æ®
    print(f"åŠ è½½æ•°æ®: {args.input_json_path}")
    with open(args.input_json_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print(f"æ¨¡å¼: {args.qa_field}")
    if args.qa_field == "annotated_qa_pairs":
        print(f"å¯è§†åŒ–æ¨¡å¼: {args.viz_mode}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print("åˆå§‹åŒ–æ¨¡å‹...")
    processor = AutoProcessor.from_pretrained(args.model_path, trust_remote_code=True)
    llm = init_llm(args.model_path, args.tensor_parallel_size, args.dtype, 
                   args.gpu_mem_util, args.max_model_len)
    
    # è¯„ä¼°
    print("å¼€å§‹è¯„ä¼°...")
    results = evaluate_physedit_with_vllm(data, args.image_base_dir, processor, llm, args)
    
    # ä¿å­˜ç»“æœ
    out_dir = os.path.dirname(args.input_json_path)
    base_name = os.path.splitext(os.path.basename(args.input_json_path))[0]
    
    # è¾“å‡ºæ–‡ä»¶å
    suffix = f"_vllm_output_{args.img_size}"
    if args.qa_field == "annotated_qa_pairs":
        suffix += f"_{args.viz_mode}"
    
    out_path = os.path.join(out_dir, base_name + suffix + ".json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # è®¡ç®—å¹¶ä¿å­˜ç»Ÿè®¡ç»“æœ
    print("è®¡ç®—ç»Ÿè®¡ç»“æœ...")
    stats = calculate_accuracy_by_dimension(results)
    
    analysis_suffix = suffix.replace("_output_", "_analysis_")
    analysis_path = os.path.join(out_dir, base_name + analysis_suffix + ".json")
    with open(analysis_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»“æœ
    print(f"\n=== è¯„ä¼°å®Œæˆ ===")
    print(f"æ€»ä½“å‡†ç¡®ç‡: {stats['overall_accuracy']:.2f}% "
          f"(samples: {stats['sample_count']}, qa_total: {stats['qa_total']})")
    print(f"\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
    for category, stat in stats["by_category"].items():
        print(f"  {category}: {stat['accuracy']:.2f}% "
              f"(samples: {stat['sample_count']}, qa_total: {stat['qa_total']})")
    print(f"\næŒ‰ç‰©ç†å®šå¾‹ç»Ÿè®¡:")
    for law, stat in stats["by_law"].items():
        print(f"  {law}: {stat['accuracy']:.2f}% "
              f"(samples: {stat['sample_count']}, qa_total: {stat['qa_total']})")
    print(f"\næŒ‰æ“ä½œç±»å‹ç»Ÿè®¡:")
    for operation, stat in stats["by_operation"].items():
        print(f"  {operation}: {stat['accuracy']:.2f}% "
              f"(samples: {stat['sample_count']}, qa_total: {stat['qa_total']})")
    
    print(f"\næ–‡ä»¶å·²ä¿å­˜:")
    print(f"  è¯¦ç»†ç»“æœ: {out_path}")
    print(f"  ç»Ÿè®¡åˆ†æ: {analysis_path}")

if __name__ == "__main__":
    random.seed(42)
    main()
